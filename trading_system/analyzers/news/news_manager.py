"""
News Manager Module
ë‰´ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œ í†µí•© ê´€ë¦¬ì
"""

import asyncio
import logging
import time
from typing import Dict, List, Optional, Any
from cachetools import TTLCache

from .news_fetcher import NewsFetcher
from .sentiment_analyzer import SentimentAnalyzer
from .news_filter import NewsFilter

# Import handling for both direct and package imports
try:
    from ...config.config import TradingConfig
    from ...database.db_manager import EnhancedDatabaseManager
    from ...utils.safe_data_handler import safe_handler
except ImportError:
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
    from config.config import TradingConfig
    from database.db_manager import EnhancedDatabaseManager
    from utils.safe_data_handler import safe_handler


class NewsManager:
    """ë‰´ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œ í†µí•© ê´€ë¦¬ì"""
    
    def __init__(self, config: TradingConfig, db: EnhancedDatabaseManager = None):
        self.config = config
        self.db = db
        self.logger = logging.getLogger(__name__)
        
        # ëª¨ë“ˆ ì´ˆê¸°í™”
        self.fetcher = NewsFetcher(config)
        self.analyzer = SentimentAnalyzer(config)
        self.filter = NewsFilter(config)
        
        # ê²°ê³¼ ìºì‹œ
        self._result_cache = TTLCache(maxsize=20, ttl=600)  # 10ë¶„ ìºì‹œ
        
        # ê²€ì¦ ì‹œìŠ¤í…œ
        self.verification_stats = {
            'total_analyses': 0,
            'successful_analyses': 0,
            'emergency_detections': 0,
            'false_alarms': 0,
            'avg_confidence': 0.0
        }
        
        # ë¹„ìš© ìµœì í™”
        self._last_analysis_time = 0
        
        # ê²€ì¦ íƒœìŠ¤í¬
        self.verification_task = None
    
    async def initialize(self):
        """ë‰´ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            # ê²€ì¦ ëª¨ë‹ˆí„°ë§ íƒœìŠ¤í¬ ì‹œì‘
            self.verification_task = asyncio.create_task(self._verification_loop())
            self.logger.info("ğŸ” ë‰´ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
        except Exception as e:
            self.logger.error(f"âŒ ë‰´ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def analyze_sentiment(self, symbol: Optional[str] = None) -> Dict[str, Any]:
        """í†µí•© ê°ì„± ë¶„ì„ ìˆ˜í–‰"""
        # ìºì‹œ í™•ì¸
        cache_key = f"news_analysis:{symbol or 'all'}"
        cached_result = self._result_cache.get(cache_key)
        if cached_result:
            return cached_result
        
        # ë¹„ìš© ìµœì í™”: ë¶„ì„ ê°„ê²© ì²´í¬
        if self._should_skip_analysis():
            return self._get_cached_or_default_result(symbol)
        
        try:
            start_time = time.time()
            
            # ë‹¨ê³„ 1: ë‰´ìŠ¤ ìˆ˜ì§‘
            self.logger.info(f"ğŸ” {symbol or 'ì „ì²´'} ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")
            news_items = await self.fetcher.fetch_news(symbol)
            
            if not news_items:
                self.logger.info(f"ğŸ“° {symbol or 'ì „ì²´'} ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ì—†ìŒ")
                result = self._get_default_sentiment()
                self._result_cache[cache_key] = result
                return result
            
            self.logger.info(f"ğŸ“° {symbol or 'ì „ì²´'} ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: {len(news_items)}ê°œ")
            
            # ë‹¨ê³„ 2: ë‰´ìŠ¤ í•„í„°ë§
            self.logger.debug(f"ğŸ§¹ ë‰´ìŠ¤ í•„í„°ë§ ì‹œì‘ ({len(news_items)}ê°œ -> ì‹ ë¢°ë„ ê²€ì¦)")
            reliable_news = self.filter.filter_reliable_news(news_items)
            
            if not reliable_news:
                self.logger.warning(f"âš ï¸ í•„í„°ë§ í›„ ì‹ ë¢°í•  ë§Œí•œ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤ (ì›ë³¸: {len(news_items)}ê°œ)")
                result = self._get_default_sentiment()
                self._result_cache[cache_key] = result
                return result
            
            filtered_ratio = len(reliable_news) / len(news_items) * 100
            self.logger.info(f"âœ… ë‰´ìŠ¤ í•„í„°ë§ ì™„ë£Œ: {len(reliable_news)}ê°œ ì„ íƒ ({filtered_ratio:.1f}% ì‹ ë¢°ë„)")
            
            # ë‹¨ê³„ 3: ê¸´ê¸‰ ìƒí™© ì²´í¬ (ê°œì„ ëœ ì•Œê³ ë¦¬ì¦˜)
            has_emergency, emergency_severity = self.filter.check_emergency_keywords(reliable_news)
            
            if has_emergency:
                self.logger.warning(f"ğŸš¨ ê¸´ê¸‰ ë‰´ìŠ¤ ê°ì§€: ì‹¬ê°ë„ {emergency_severity}")
            else:
                self.logger.debug(f"âœ… ê¸´ê¸‰ ìƒí™© ì—†ìŒ: ì •ìƒ ë¶„ì„ ì§„í–‰")
            
            # ë‹¨ê³„ 4: ê°ì„± ë¶„ì„
            self.logger.info(f"ğŸ’­ ê°ì„± ë¶„ì„ ì‹œì‘ ({len(reliable_news)}ê°œ ë‰´ìŠ¤)")
            sentiment_result = await self.analyzer.analyze_sentiment(reliable_news, symbol)
            
            # ì›ì‹œ ê°ì„± ì ìˆ˜ ë¡œê¹…
            raw_sentiment = sentiment_result.get('sentiment_score', 0.0)
            self.logger.debug(f"ğŸ“Š ì›ì‹œ ê°ì„± ì ìˆ˜: {raw_sentiment:.3f}")
            
            # ë‹¨ê³„ 5: ê²°ê³¼ í†µí•© ë° ì¡°ì •
            final_result = self._integrate_analysis_results(
                sentiment_result, has_emergency, emergency_severity, reliable_news
            )
            
            # NEWS_WEIGHT ì ìš©ëœ ìµœì¢… ì ìˆ˜ ë¡œê¹…
            final_sentiment = final_result.get('sentiment_score', 0.0)
            news_weight = self.config.NEWS_WEIGHT
            weighted_sentiment = final_sentiment * news_weight
            
            self.logger.info(f"ğŸ“ˆ ê°ì„± ë¶„ì„ ê²°ê³¼: ì›ì‹œ={raw_sentiment:.3f}, ìµœì¢…={final_sentiment:.3f}")
            self.logger.info(f"âš–ï¸ ê°€ì¤‘ì¹˜ ì ìš©: {final_sentiment:.3f} Ã— {news_weight} = {weighted_sentiment:.3f}")
            
            # ê°€ì¤‘ì¹˜ ì ìš©ëœ ì ìˆ˜ë¥¼ ê²°ê³¼ì— ì¶”ê°€
            final_result['weighted_sentiment'] = weighted_sentiment
            final_result['news_weight_applied'] = news_weight
            
            # ì²˜ë¦¬ ì‹œê°„ ê¸°ë¡
            processing_time = time.time() - start_time
            final_result['processing_time'] = processing_time
            final_result['news_count'] = len(reliable_news)
            
            # ìºì‹œ ì €ì¥
            self._result_cache[cache_key] = final_result
            
            # ì•ˆì „ ì²˜ë¦¬ëœ ê²°ê³¼ë¡œ í†µê³„ ì—…ë°ì´íŠ¸
            safe_result = safe_handler.ensure_analysis_result_keys(final_result)
            self._update_verification_stats(safe_result)
            
            # ë¶„ì„ ì‹œê°„ ì—…ë°ì´íŠ¸
            self._last_analysis_time = time.time()
            
            self.logger.info(f"âœ… {symbol or 'ì „ì²´'} ë‰´ìŠ¤ ë¶„ì„ ì™„ë£Œ ({processing_time:.1f}ì´ˆ)")
            return final_result
            
        except Exception as e:
            self.logger.error(f"ë‰´ìŠ¤ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return self._get_default_sentiment()
    
    def _should_skip_analysis(self) -> bool:
        """ë¶„ì„ì„ ê±´ë„ˆë›¸ì§€ ê²°ì • (ë¹„ìš© ìµœì í™”)"""
        if not (hasattr(self.config, 'ENABLE_COST_OPTIMIZATION') and self.config.ENABLE_COST_OPTIMIZATION):
            return False
        
        current_time = time.time()
        interval = getattr(self.config, 'NEWS_ANALYSIS_INTERVAL', 900)  # ê¸°ë³¸ 15ë¶„
        
        return (current_time - self._last_analysis_time) < interval
    
    def _get_cached_or_default_result(self, symbol: Optional[str]) -> Dict[str, Any]:
        """ìºì‹œëœ ê²°ê³¼ ë˜ëŠ” ê¸°ë³¸ê°’ ë°˜í™˜"""
        # ìµœê·¼ ê²°ê³¼ ì°¾ê¸°
        for cache_key in self._result_cache.keys():
            if symbol and symbol in cache_key:
                return self._result_cache[cache_key]
            elif not symbol and 'all' in cache_key:
                return self._result_cache[cache_key]
        
        return self._get_default_sentiment()
    
    def _integrate_analysis_results(
        self, 
        sentiment_result: Dict[str, Any], 
        has_emergency: bool, 
        emergency_severity: float,
        news_items: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """ë¶„ì„ ê²°ê³¼ë“¤ì„ í†µí•©í•˜ì—¬ ìµœì¢… ê²°ê³¼ ìƒì„±"""
        
        # ê¸°ë³¸ ê°ì„± ê²°ê³¼ ë³µì‚¬
        result = sentiment_result.copy()
        
        # ê¸´ê¸‰ ìƒí™© ì •ë³´ ì¶”ê°€
        result['emergency'] = {
            'detected': has_emergency,
            'severity': emergency_severity,
            'threshold': 1.2  # í˜„ì¬ ì„ê³„ê°’
        }
        
        # ê¸´ê¸‰ ìƒí™©ì¼ ë•Œ ê°ì„± ì ìˆ˜ ì¡°ì • (í•˜ì§€ë§Œ ê³¼ë¯¼ë°˜ì‘ ë°©ì§€)
        if has_emergency and emergency_severity >= 1.5:  # ë” ë†’ì€ ì„ê³„ê°’
            # ê°ì„± ì ìˆ˜ë¥¼ ë” ë³´ìˆ˜ì ìœ¼ë¡œ ì¡°ì •
            adjustment_factor = min(0.3, emergency_severity * 0.1)  # ìµœëŒ€ 30% ì¡°ì •
            
            if result['sentiment'] > 0:
                result['sentiment'] = max(0, result['sentiment'] - adjustment_factor)
            else:
                result['sentiment'] = min(0, result['sentiment'] - adjustment_factor)
            
            # ì‹ ë¢°ë„ ì¦ê°€ (ê¸´ê¸‰ ìƒí™©ì€ ì¤‘ìš”)
            result['confidence'] = min(1.0, result['confidence'] + 0.2)
            
            # ì˜í–¥ ìˆ˜ì¤€ ìƒí–¥ ì¡°ì •
            if result['impact'] == 'low':
                result['impact'] = 'medium'
            elif result['impact'] == 'medium':
                result['impact'] = 'high'
        
        # ë‰´ìŠ¤ ë©”íƒ€ë°ì´í„° ì¶”ê°€
        result['news_metadata'] = {
            'total_sources': len(set(item.get('source', '') for item in news_items)),
            'avg_reliability': sum(item.get('source_reliability', 0.5) for item in news_items) / len(news_items),
            'time_range_hours': self._calculate_time_range(news_items),
            'top_sources': self._get_top_sources(news_items)
        }
        
        return result
    
    def _calculate_time_range(self, news_items: List[Dict[str, Any]]) -> float:
        """ë‰´ìŠ¤ë“¤ì˜ ì‹œê°„ ë²”ìœ„ ê³„ì‚° (ì‹œê°„ ë‹¨ìœ„)"""
        if not news_items:
            return 0.0
        
        timestamps = [item.get('timestamp', time.time()) for item in news_items]
        return (max(timestamps) - min(timestamps)) / 3600.0  # ì‹œê°„ ë‹¨ìœ„ë¡œ ë³€í™˜
    
    def _get_top_sources(self, news_items: List[Dict[str, Any]]) -> List[str]:
        """ì£¼ìš” ë‰´ìŠ¤ ì†ŒìŠ¤ ëª©ë¡ ë°˜í™˜"""
        source_counts = {}
        
        for item in news_items:
            source = item.get('source', 'Unknown')
            source_counts[source] = source_counts.get(source, 0) + 1
        
        # ë¹ˆë„ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ 3ê°œ ë°˜í™˜
        sorted_sources = sorted(source_counts.items(), key=lambda x: x[1], reverse=True)
        return [source for source, count in sorted_sources[:3]]
    
    def _get_default_sentiment(self) -> Dict[str, Any]:
        """ê¸°ë³¸ ì¤‘ì„± ê°ì„± ë°˜í™˜"""
        return {
            'sentiment': 0.0,
            'impact': 'low',
            'confidence': 0.1,
            'keywords': [],
            'summary': 'ë¶„ì„ ê°€ëŠ¥í•œ ë‰´ìŠ¤ ì—†ìŒ',
            'analysis_type': 'default',
            'emergency': {'detected': False, 'severity': 0.0, 'threshold': 1.2},
            'news_metadata': {
                'total_sources': 0,
                'avg_reliability': 0.0,
                'time_range_hours': 0.0,
                'top_sources': []
            },
            'processing_time': 0.0,
            'news_count': 0
        }
    
    async def _verification_loop(self):
        """ë‰´ìŠ¤ ë¶„ì„ í’ˆì§ˆ ê²€ì¦ ë£¨í”„"""
        while True:
            try:
                await asyncio.sleep(3600)  # 1ì‹œê°„ë§ˆë‹¤ ê²€ì¦
                
                # ë¶„ì„ í†µê³„ ìˆ˜ì§‘
                stats = self.get_comprehensive_stats()
                
                if self.db:
                    await self._log_verification_results(stats)
                
                self.logger.info(f"ğŸ“Š ë‰´ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œ ìƒíƒœ: ì„±ê³µë¥  {stats['success_rate']:.1%}")
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                self.logger.error(f"ê²€ì¦ ë£¨í”„ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(300)  # 5ë¶„ í›„ ì¬ì‹œë„
    
    async def _log_verification_results(self, stats: Dict[str, Any]):
        """ê²€ì¦ ê²°ê³¼ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥"""
        try:
            if self.db:
                self.db.log_system_event(
                    level="INFO",
                    component="NewsAnalysis",
                    message="ë‰´ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¦¬í¬íŠ¸",
                    details=stats
                )
        except Exception as e:
            self.logger.debug(f"ê²€ì¦ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _update_verification_stats(self, result: Dict[str, Any]):
        """ê²€ì¦ í†µê³„ ì—…ë°ì´íŠ¸ - SafeDataHandler ì ìš©"""
        try:
            # ì•ˆì „í•œ ë°ì´í„° ì ‘ê·¼
            result = safe_handler.ensure_analysis_result_keys(result)
            
            self.verification_stats['total_analyses'] += 1
            
            if safe_handler.safe_get(result, 'confidence', 0) > 0.3:
                self.verification_stats['successful_analyses'] += 1
            
            if safe_handler.safe_get(result, 'emergency', {}).get('detected', False):
                self.verification_stats['emergency_detections'] += 1
            
            # í‰ê·  ì‹ ë¢°ë„ ì—…ë°ì´íŠ¸ - KeyError ë°©ì§€
            total = self.verification_stats['total_analyses']
            current_avg = safe_handler.safe_get(self.verification_stats, 'avg_confidence', 0.5)
            new_confidence = safe_handler.safe_get(result, 'avg_confidence', 0.5)
            
            self.verification_stats['avg_confidence'] = (
                (current_avg * (total - 1) + new_confidence) / total
            )
            
        except Exception as e:
            self.logger.debug(f"í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def get_comprehensive_stats(self) -> Dict[str, Any]:
        """ì¢…í•© í†µê³„ ë°˜í™˜ - SafeDataHandler ì ìš©"""
        total = max(self.verification_stats['total_analyses'], 1)
        
        stats = {
            'success_rate': self.verification_stats['successful_analyses'] / total,
            'avg_confidence': safe_handler.safe_get(self.verification_stats, 'avg_confidence', 0.5),
            'emergency_detection_rate': self.verification_stats['emergency_detections'] / total,
            'total_analyses': total,
            'fetcher_stats': self.fetcher.get_source_stats(),
            'analyzer_stats': self.analyzer.get_analysis_stats(),
            'filter_stats': self.filter.get_filter_stats(),
            'cache_hit_ratio': len(self._result_cache) / max(total, 1)
        }
        
        # ì•ˆì „í•œ ê²°ê³¼ í‚¤ ë³´ì¥
        return safe_handler.ensure_analysis_result_keys(stats)
    
    async def cleanup(self):
        """ì‹œìŠ¤í…œ ì •ë¦¬"""
        try:
            if self.verification_task:
                self.verification_task.cancel()
                try:
                    await self.verification_task
                except asyncio.CancelledError:
                    pass
            
            self.logger.info("ë‰´ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"ì‹œìŠ¤í…œ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")