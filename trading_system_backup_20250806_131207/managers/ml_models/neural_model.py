"""
Neural Model
ì‹ ê²½ë§ ëª¨ë¸ (MLP, Deep Learning ë“±)
"""

import numpy as np
from typing import Dict, Any, Tuple
from .base_model import BaseModel

try:
    from sklearn.neural_network import MLPRegressor
    from sklearn.model_selection import validation_curve
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False


class NeuralModel(BaseModel):
    """ì‹ ê²½ë§ ê¸°ë°˜ ëª¨ë¸ í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str, config, db, hidden_layers: Tuple[int, ...] = (100, 50)):
        super().__init__(model_name, config, db)
        self.hidden_layers = hidden_layers
        self.training_history = []
        
    def _create_model(self):
        """ì‹ ê²½ë§ ëª¨ë¸ ìƒì„±"""
        if not ML_AVAILABLE:
            return None
            
        return MLPRegressor(
            hidden_layer_sizes=self.hidden_layers,
            activation='relu',
            solver='adam',
            alpha=0.001,
            learning_rate='adaptive',
            learning_rate_init=0.001,
            max_iter=500,
            early_stopping=True,
            validation_fraction=0.1,
            n_iter_no_change=10,
            random_state=42
        )
    
    def get_model_params(self) -> Dict[str, Any]:
        """ëª¨ë¸ íŒŒë¼ë¯¸í„° ë°˜í™˜"""
        return {
            'model_type': 'neural_network',
            'hidden_layers': self.hidden_layers,
            'sklearn_params': self.model.get_params() if self.model else {},
            'training_iterations': getattr(self.model, 'n_iter_', 0) if self.model else 0
        }
    
    def train(self, X: np.ndarray, y: np.ndarray) -> Dict[str, Any]:
        """ì‹ ê²½ë§ ëª¨ë¸ í›ˆë ¨"""
        try:
            # ê¸°ë³¸ í›ˆë ¨ ìˆ˜í–‰
            result = super().train(X, y)
            
            if result['success'] and self.model:
                # í›ˆë ¨ íˆìŠ¤í† ë¦¬ ì €ì¥
                training_info = {
                    'n_iter': getattr(self.model, 'n_iter_', 0),
                    'loss': getattr(self.model, 'loss_', float('inf')),
                    'validation_scores': getattr(self.model, 'validation_scores_', []),
                    'training_time': result.get('training_time', 0)
                }
                self.training_history.append(training_info)
                
                # ìˆ˜ë ´ ì—¬ë¶€ í™•ì¸
                if hasattr(self.model, 'n_iter_') and hasattr(self.model, 'max_iter'):
                    converged = self.model.n_iter_ < self.model.max_iter
                    result['converged'] = converged
                    
                    if not converged:
                        self.logger.warning(f"âš ï¸ {self.model_name} ëª¨ë¸ì´ ìˆ˜ë ´í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                
                result['training_info'] = training_info
            
            return result
            
        except Exception as e:
            self.logger.error(f"ì‹ ê²½ë§ í›ˆë ¨ ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}
    
    def optimize_architecture(self, X, y, max_layers: int = 3, max_neurons: int = 200) -> Dict[str, Any]:
        """ì‹ ê²½ë§ êµ¬ì¡° ìµœì í™”"""
        try:
            if not ML_AVAILABLE or len(X) < 100:  # ì¶©ë¶„í•œ ë°ì´í„° í•„ìš”
                return {'success': False, 'error': 'Insufficient data for architecture optimization'}
            
            best_score = float('inf')
            best_architecture = self.hidden_layers
            
            # ë‹¤ì–‘í•œ êµ¬ì¡° í…ŒìŠ¤íŠ¸
            architectures_to_test = [
                (50,),
                (100,),
                (100, 50),
                (100, 100),
                (200, 100),
                (100, 50, 25),
                (200, 100, 50)
            ]
            
            results = []
            
            for architecture in architectures_to_test:
                try:
                    # ì„ì‹œ ëª¨ë¸ ìƒì„±
                    temp_model = MLPRegressor(
                        hidden_layer_sizes=architecture,
                        activation='relu',
                        solver='adam',
                        alpha=0.001,
                        max_iter=200,  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì¤„ì„
                        early_stopping=True,
                        validation_fraction=0.2,
                        random_state=42
                    )
                    
                    # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
                    X_scaled = self.scaler.fit_transform(X)
                    
                    # í›ˆë ¨ ë° í‰ê°€
                    temp_model.fit(X_scaled, y)
                    score = temp_model.loss_
                    
                    results.append({
                        'architecture': architecture,
                        'score': score,
                        'n_iter': temp_model.n_iter_,
                        'converged': temp_model.n_iter_ < temp_model.max_iter
                    })
                    
                    if score < best_score:
                        best_score = score
                        best_architecture = architecture
                    
                except Exception as e:
                    self.logger.debug(f"êµ¬ì¡° {architecture} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
                    continue
            
            # ìµœì  êµ¬ì¡°ë¡œ ì—…ë°ì´íŠ¸
            if best_architecture != self.hidden_layers:
                self.hidden_layers = best_architecture
                self.model = self._create_model()
                
                self.logger.info(f"âœ… {self.model_name} ìµœì  êµ¬ì¡°: {best_architecture}")
            
            return {
                'success': True,
                'best_architecture': best_architecture,
                'best_score': best_score,
                'all_results': results
            }
            
        except Exception as e:
            self.logger.error(f"êµ¬ì¡° ìµœì í™” ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}
    
    def analyze_learning_curve(self, X, y) -> Dict[str, Any]:
        """í•™ìŠµ ê³¡ì„  ë¶„ì„"""
        try:
            if not ML_AVAILABLE:
                return {'success': False, 'error': 'ML libraries not available'}
            
            # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
            X_scaled = self.scaler.fit_transform(X)
            
            # ë‹¤ì–‘í•œ max_iter ê°’ì— ëŒ€í•´ ê²€ì¦ ê³¡ì„  ìƒì„±
            param_range = [50, 100, 200, 300, 500]
            
            train_scores, validation_scores = validation_curve(
                MLPRegressor(
                    hidden_layer_sizes=self.hidden_layers,
                    activation='relu',
                    solver='adam',
                    alpha=0.001,
                    early_stopping=False,  # ê³¡ì„  ë¶„ì„ì„ ìœ„í•´ ë¹„í™œì„±í™”
                    random_state=42
                ),
                X_scaled, y,
                param_name='max_iter',
                param_range=param_range,
                cv=3,
                scoring='neg_mean_squared_error',
                n_jobs=-1
            )
            
            # ê²°ê³¼ ì •ë¦¬
            train_mean = -train_scores.mean(axis=1)
            train_std = train_scores.std(axis=1)
            val_mean = -validation_scores.mean(axis=1)
            val_std = validation_scores.std(axis=1)
            
            return {
                'success': True,
                'param_range': param_range,
                'train_scores': {
                    'mean': train_mean.tolist(),
                    'std': train_std.tolist()
                },
                'validation_scores': {
                    'mean': val_mean.tolist(),
                    'std': val_std.tolist()
                },
                'optimal_iterations': param_range[np.argmin(val_mean)]
            }
            
        except Exception as e:
            self.logger.error(f"í•™ìŠµ ê³¡ì„  ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}
    
    def _calculate_prediction_confidence(self, X_scaled):
        """ì‹ ê²½ë§ ëª¨ë¸ì˜ ì˜ˆì¸¡ ì‹ ë¢°ë„ ê³„ì‚°"""
        try:
            # ê¸°ë³¸ ì‹ ë¢°ë„
            base_confidence = super()._calculate_prediction_confidence(X_scaled)
            
            # ì‹ ê²½ë§ íŠ¹ì„±ì„ í™œìš©í•œ ì‹ ë¢°ë„ ì¡°ì •
            if hasattr(self.model, 'loss_'):
                # í›ˆë ¨ ì†ì‹¤ì„ ê¸°ë°˜ìœ¼ë¡œ ì‹ ë¢°ë„ ì¡°ì •
                loss = self.model.loss_
                
                # ì†ì‹¤ì´ ë‚®ì„ìˆ˜ë¡ ì‹ ë¢°ë„ ë†’ìŒ
                loss_confidence = max(0.0, 1.0 - min(1.0, loss / 10.0))
                
                # ìˆ˜ë ´ ì—¬ë¶€ë„ ê³ ë ¤
                converged = True
                if hasattr(self.model, 'n_iter_') and hasattr(self.model, 'max_iter'):
                    converged = self.model.n_iter_ < self.model.max_iter
                
                convergence_bonus = 0.1 if converged else -0.1
                
                final_confidence = 0.6 * base_confidence + 0.3 * loss_confidence + convergence_bonus
                return min(1.0, max(0.0, final_confidence))
            
            return base_confidence
            
        except Exception as e:
            self.logger.debug(f"ì‹ ë¢°ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def get_network_info(self) -> Dict[str, Any]:
        """ì‹ ê²½ë§ ì •ë³´ ë°˜í™˜"""
        info = self.get_performance_summary()
        
        if self.model:
            info['network_info'] = {
                'hidden_layers': self.hidden_layers,
                'n_layers': len(self.hidden_layers) + 2,  # hidden + input + output
                'total_parameters': self._estimate_parameters(),
                'activation': getattr(self.model, 'activation', 'relu'),
                'solver': getattr(self.model, 'solver', 'adam'),
                'learning_rate_init': getattr(self.model, 'learning_rate_init', 0.001)
            }
            
            if hasattr(self.model, 'n_iter_'):
                info['training_info'] = {
                    'iterations': self.model.n_iter_,
                    'loss': getattr(self.model, 'loss_', None),
                    'converged': self.model.n_iter_ < getattr(self.model, 'max_iter', 500)
                }
        
        info['training_history'] = self.training_history
        
        return info
    
    def _estimate_parameters(self) -> int:
        """ì‹ ê²½ë§ íŒŒë¼ë¯¸í„° ìˆ˜ ì¶”ì •"""
        try:
            if not self.feature_names:
                return 0
            
            input_size = len(self.feature_names)
            total_params = 0
            
            prev_size = input_size
            for hidden_size in self.hidden_layers:
                total_params += prev_size * hidden_size + hidden_size  # weights + biases
                prev_size = hidden_size
            
            # ì¶œë ¥ì¸µ
            total_params += prev_size * 1 + 1  # output layer weights + bias
            
            return total_params
            
        except Exception:
            return 0
    
    def reset_training_history(self):
        """í›ˆë ¨ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.training_history = []
        self.logger.info(f"ğŸ”„ {self.model_name} í›ˆë ¨ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”")