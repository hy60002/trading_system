"""
Base Model Class
ëª¨ë“  ML ëª¨ë¸ì˜ ê¸°ë³¸ í´ë˜ìŠ¤
"""

import logging
import numpy as np
import pandas as pd
import joblib
import os
from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime

# ML libraries
try:
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False

# Import handling for both direct and package imports
try:
    from ...config.config import TradingConfig
    from ...database.db_manager import EnhancedDatabaseManager
except ImportError:
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
    from config.config import TradingConfig
    from database.db_manager import EnhancedDatabaseManager


class BaseModel(ABC):
    """ëª¨ë“  ML ëª¨ë¸ì˜ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str, config: TradingConfig, db: EnhancedDatabaseManager):
        self.model_name = model_name
        self.config = config
        self.db = db
        self.logger = logging.getLogger(f"{self.__class__.__name__}_{model_name}")
        
        # ëª¨ë¸ ìƒíƒœ
        self.model = None
        self.scaler = None
        self.is_trained = False
        self.last_train_time = None
        
        # ì„±ëŠ¥ ì¶”ì 
        self.performance_metrics = {
            'mse': float('inf'),
            'mae': float('inf'),
            'r2': -float('inf'),
            'accuracy': 0.0,
            'prediction_count': 0,
            'last_update': None
        }
        
        # íŠ¹ì„± ì •ë³´
        self.feature_names = []
        self.feature_importance = {}
        
        # ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
        self.model_dir = "models"
        if not os.path.exists(self.model_dir):
            os.makedirs(self.model_dir)
        
        self.model_path = os.path.join(self.model_dir, f"{self.model_name}_model.pkl")
        self.scaler_path = os.path.join(self.model_dir, f"{self.model_name}_scaler.pkl")
    
    @abstractmethod
    def _create_model(self) -> Any:
        """ëª¨ë¸ ìƒì„± (ê° ëª¨ë¸ì—ì„œ êµ¬í˜„)"""
        pass
    
    @abstractmethod
    def get_model_params(self) -> Dict[str, Any]:
        """ëª¨ë¸ íŒŒë¼ë¯¸í„° ë°˜í™˜ (ê° ëª¨ë¸ì—ì„œ êµ¬í˜„)"""
        pass
    
    def initialize(self) -> bool:
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            if ML_AVAILABLE:
                # ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ ì‹œë„
                if self.load_model():
                    self.logger.info(f"âœ… {self.model_name} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                    return True
                else:
                    # ìƒˆ ëª¨ë¸ ìƒì„±
                    self.model = self._create_model()
                    self.scaler = StandardScaler()
                    self.logger.info(f"ğŸ†• {self.model_name} ìƒˆ ëª¨ë¸ ìƒì„± ì™„ë£Œ")
                    return True
            else:
                self.logger.warning(f"âŒ ML ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ - {self.model_name} ëª¨ë¸ ë¹„í™œì„±í™”")
                return False
        except Exception as e:
            self.logger.error(f"âŒ {self.model_name} ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def prepare_features(self, data: pd.DataFrame) -> Tuple[np.ndarray, List[str]]:
        """íŠ¹ì„± ì¤€ë¹„ ë° ì „ì²˜ë¦¬"""
        try:
            # ê¸°ë³¸ íŠ¹ì„±ë“¤
            features = []
            feature_names = []
            
            # ê¸°ìˆ ì  ì§€í‘œë“¤
            if 'rsi' in data.columns:
                features.append(data['rsi'].values)
                feature_names.append('rsi')
            
            if 'macd' in data.columns:
                features.append(data['macd'].values)
                feature_names.append('macd')
            
            if 'bb_position' in data.columns:
                features.append(data['bb_position'].values)
                feature_names.append('bb_position')
            
            if 'volume' in data.columns:
                features.append(data['volume'].values)
                feature_names.append('volume')
            
            # ê°€ê²© ê¸°ë°˜ íŠ¹ì„±
            if 'close' in data.columns:
                close_prices = data['close'].values
                if len(close_prices) > 1:
                    price_change = np.diff(close_prices, prepend=close_prices[0])
                    features.append(price_change)
                    feature_names.append('price_change')
            
            # íŠ¹ì„± ë°°ì—´ ìƒì„±
            if features:
                X = np.column_stack(features)
                self.feature_names = feature_names
                return X, feature_names
            else:
                # ìµœì†Œí•œì˜ íŠ¹ì„±ì´ë¼ë„ ìƒì„±
                X = np.random.random((len(data), 1))
                feature_names = ['dummy_feature']
                self.feature_names = feature_names
                return X, feature_names
                
        except Exception as e:
            self.logger.error(f"íŠ¹ì„± ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            # ë”ë¯¸ ë°ì´í„° ë°˜í™˜
            X = np.random.random((len(data), 1))
            feature_names = ['dummy_feature']
            return X, feature_names
    
    def train(self, X: np.ndarray, y: np.ndarray) -> Dict[str, Any]:
        """ëª¨ë¸ í›ˆë ¨"""
        try:
            if not ML_AVAILABLE or self.model is None:
                return {'success': False, 'error': 'Model not available'}
            
            # ë°ì´í„° ê²€ì¦
            if len(X) < 10:
                return {'success': False, 'error': 'Insufficient training data'}
            
            # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
            X_scaled = self.scaler.fit_transform(X)
            
            # ëª¨ë¸ í›ˆë ¨
            self.model.fit(X_scaled, y)
            self.is_trained = True
            self.last_train_time = datetime.now()
            
            # ì„±ëŠ¥ í‰ê°€
            y_pred = self.model.predict(X_scaled)
            metrics = self._calculate_metrics(y, y_pred)
            self.performance_metrics.update(metrics)
            self.performance_metrics['last_update'] = datetime.now()
            
            # íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚° (ê°€ëŠ¥í•œ ê²½ìš°)
            if hasattr(self.model, 'feature_importances_'):
                self.feature_importance = dict(zip(
                    self.feature_names, 
                    self.model.feature_importances_
                ))
            
            self.logger.info(f"âœ… {self.model_name} í›ˆë ¨ ì™„ë£Œ - RÂ²: {metrics['r2']:.3f}")
            return {'success': True, 'metrics': metrics}
            
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}
    
    def predict(self, X: np.ndarray) -> Dict[str, Any]:
        """ì˜ˆì¸¡ ìˆ˜í–‰"""
        try:
            if not self.is_trained or self.model is None:
                return {'success': False, 'error': 'Model not trained'}
            
            # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
            X_scaled = self.scaler.transform(X)
            
            # ì˜ˆì¸¡
            prediction = self.model.predict(X_scaled)
            
            # ì‹ ë¢°ë„ ê³„ì‚° (ê°€ëŠ¥í•œ ê²½ìš°)
            confidence = self._calculate_prediction_confidence(X_scaled)
            
            # ì˜ˆì¸¡ ì¹´ìš´íŠ¸ ì¦ê°€
            self.performance_metrics['prediction_count'] += 1
            
            return {
                'success': True,
                'prediction': float(prediction[0]) if len(prediction) == 1 else prediction.tolist(),
                'confidence': confidence,
                'model_name': self.model_name
            }
            
        except Exception as e:
            self.logger.error(f"ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}
    
    def _calculate_metrics(self, y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        try:
            metrics = {
                'mse': mean_squared_error(y_true, y_pred),
                'mae': mean_absolute_error(y_true, y_pred),
                'r2': r2_score(y_true, y_pred)
            }
            
            # ì •í™•ë„ ê³„ì‚° (ë¶„ë¥˜ ë¬¸ì œë¡œ ê°„ì£¼)
            threshold = 0.01  # 1% ì´ë‚´ë©´ ì •í™•í•œ ì˜ˆì¸¡ìœ¼ë¡œ ê°„ì£¼
            accurate_predictions = np.abs(y_true - y_pred) <= threshold
            metrics['accuracy'] = np.mean(accurate_predictions)
            
            return metrics
        except Exception as e:
            self.logger.debug(f"ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {'mse': float('inf'), 'mae': float('inf'), 'r2': -1.0, 'accuracy': 0.0}
    
    def _calculate_prediction_confidence(self, X_scaled: np.ndarray) -> float:
        """ì˜ˆì¸¡ ì‹ ë¢°ë„ ê³„ì‚°"""
        try:
            # ê¸°ë³¸ì ì¸ ì‹ ë¢°ë„ ê³„ì‚° (ëª¨ë¸ ì„±ëŠ¥ ê¸°ë°˜)
            r2_score = self.performance_metrics.get('r2', -1.0)
            base_confidence = max(0.0, min(1.0, (r2_score + 1.0) / 2.0))  # -1~1ì„ 0~1ë¡œ ë³€í™˜
            
            return base_confidence
        except Exception:
            return 0.5  # ê¸°ë³¸ ì‹ ë¢°ë„
    
    def save_model(self) -> bool:
        """ëª¨ë¸ ì €ì¥"""
        try:
            if self.model is not None:
                joblib.dump(self.model, self.model_path)
            
            if self.scaler is not None:
                joblib.dump(self.scaler, self.scaler_path)
            
            self.logger.info(f"âœ… {self.model_name} ëª¨ë¸ ì €ì¥ ì™„ë£Œ")
            return True
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def load_model(self) -> bool:
        """ëª¨ë¸ ë¡œë“œ"""
        try:
            if os.path.exists(self.model_path) and os.path.exists(self.scaler_path):
                self.model = joblib.load(self.model_path)
                self.scaler = joblib.load(self.scaler_path)
                self.is_trained = True
                self.last_train_time = datetime.fromtimestamp(os.path.getmtime(self.model_path))
                return True
            return False
        except Exception as e:
            self.logger.debug(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìš”ì•½ ë°˜í™˜"""
        return {
            'model_name': self.model_name,
            'is_trained': self.is_trained,
            'last_train_time': self.last_train_time,
            'metrics': self.performance_metrics.copy(),
            'feature_count': len(self.feature_names),
            'feature_names': self.feature_names.copy(),
            'feature_importance': self.feature_importance.copy() if self.feature_importance else {}
        }
    
    def should_retrain(self) -> bool:
        """ì¬í›ˆë ¨ì´ í•„ìš”í•œì§€ í™•ì¸"""
        if not self.is_trained:
            return True
        
        if self.last_train_time is None:
            return True
        
        # ì„¤ì •ëœ ì¬í›ˆë ¨ ì£¼ê¸° í™•ì¸
        hours_since_train = (datetime.now() - self.last_train_time).total_seconds() / 3600
        retrain_hours = getattr(self.config, 'ML_RETRAIN_HOURS', 24)
        
        return hours_since_train >= retrain_hours
    
    def reset_model(self):
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        self.model = self._create_model()
        self.scaler = StandardScaler()
        self.is_trained = False
        self.last_train_time = None
        self.performance_metrics = {
            'mse': float('inf'),
            'mae': float('inf'), 
            'r2': -float('inf'),
            'accuracy': 0.0,
            'prediction_count': 0,
            'last_update': None
        }
        self.feature_importance = {}
        self.logger.info(f"ğŸ”„ {self.model_name} ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")