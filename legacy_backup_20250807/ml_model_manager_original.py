"""
Enhanced ML Model Manager
Machine learning model management with real-time learning and performance tracking
"""

import asyncio
import logging
import os
import json
import joblib
import numpy as np
import pandas as pd
from datetime import datetime
from collections import defaultdict
from typing import Dict, List, Optional, Any

# ML libraries
try:
    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
    from sklearn.neural_network import MLPRegressor
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    import xgboost as xgb
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False

# Import handling for both direct and package imports
try:
    from ..config.config import TradingConfig
    from ..database.db_manager import EnhancedDatabaseManager
except ImportError:
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from config.config import TradingConfig
    from database.db_manager import EnhancedDatabaseManager


class EnhancedMLModelManager:
    """Machine learning model management with real-time learning and performance tracking"""
    
    def __init__(self, config: TradingConfig, db: EnhancedDatabaseManager):
        self.config = config
        self.db = db
        self.logger = logging.getLogger(__name__)
        self.models = {}
        self.scalers = {}
        self.feature_names = []
        self.model_performance = {}
        self.last_retrain_time = {}
        
        # ğŸ” Prediction verification system
        self.pending_predictions = {}  # ê²€ì¦ ëŒ€ê¸° ì¤‘ì¸ ì˜ˆì¸¡ë“¤
        self.verification_task = None
        self._is_trained = False
        
        if not ML_AVAILABLE:
            self.logger.warning("ML ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ML ê¸°ëŠ¥ì„ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")
            self.config.ENABLE_ML_MODELS = False
            return
        
        if hasattr(config, 'ENABLE_ML_MODELS') and config.ENABLE_ML_MODELS:
            self._initialize_models()
    
    async def initialize(self):
        """Initialize ML models (async compatibility)"""
        # Already initialized in __init__, this is for compatibility
        
        # Start prediction verification task
        if ML_AVAILABLE:
            self.verification_task = asyncio.create_task(self._prediction_verification_loop())
            self.logger.info("ğŸ” ML ì˜ˆì¸¡ ê²€ì¦ ì‹œìŠ¤í…œ ì‹œì‘")
        
        return True
    
    def _initialize_models(self):
        """Initialize ML models and scalers"""
        try:
            # Feature configuration
            self.feature_names = [
                'returns_1', 'returns_5', 'returns_20',
                'volume_ratio', 'atr_ratio', 'bb_position',
                'rsi', 'macd_signal', 'adx', 'obv_slope',
                'price_ma_ratio', 'volume_ma_ratio',
                'trend_strength', 'volatility_ratio'
            ]
            
            # Initialize scalers
            self.scalers['features'] = StandardScaler()
            self.scalers['target'] = MinMaxScaler()
            
            # Try to load existing models
            self._load_models()
            
            # If no models exist, create default ones
            if not self.models:
                self._create_default_models()
                # Note: Initial training will be triggered later during first use
                
        except Exception as e:
            self.logger.error(f"ML ëª¨ë¸ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            self.config.ENABLE_ML_MODELS = False
    
    def _load_models(self):
        """Load saved models"""
        model_dir = "models"
        if not os.path.exists(model_dir):
            os.makedirs(model_dir)
            return
        
        # Load models
        model_files = {
            'random_forest': 'random_forest_model.pkl',
            'gradient_boost': 'gradient_boost_model.pkl',
            'neural_network': 'neural_network_model.pkl',
            'xgboost': 'xgboost_model.pkl'
        }
        
        for model_name, filename in model_files.items():
            filepath = os.path.join(model_dir, filename)
            if os.path.exists(filepath):
                try:
                    self.models[model_name] = joblib.load(filepath)
                    self.logger.info(f"{model_name} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                    self._is_trained = True
                    
                    # Load performance history
                    perf = self.db.get_ml_model_performance(model_name)
                    self.model_performance[model_name] = perf
                    
                except Exception as e:
                    self.logger.error(f"{model_name} ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # Load scalers
        scaler_path = os.path.join(model_dir, 'scalers.pkl')
        if os.path.exists(scaler_path):
            try:
                self.scalers = joblib.load(scaler_path)
            except Exception as e:
                self.logger.warning(f"ML ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë”© ì‹¤íŒ¨: {e}")
    
    def _create_default_models(self):
        """Create default models"""
        # Random Forest
        self.models['random_forest'] = RandomForestRegressor(
            n_estimators=100,
            max_depth=10,
            min_samples_split=20,
            random_state=42,
            n_jobs=-1
        )
        
        # Gradient Boosting
        self.models['gradient_boost'] = GradientBoostingRegressor(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=5,
            random_state=42
        )
        
        # Neural Network
        self.models['neural_network'] = MLPRegressor(
            hidden_layer_sizes=(100, 50, 25),
            activation='relu',
            solver='adam',
            alpha=0.001,
            random_state=42,
            max_iter=500
        )
        
        # XGBoost
        self.models['xgboost'] = xgb.XGBRegressor(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            random_state=42
        )
        
        self.logger.info("ê¸°ë³¸ ML ëª¨ë¸ ìƒì„± ì™„ë£Œ")
    
    async def get_predictions(self, symbol: str, features: Dict[str, float]) -> Dict[str, Any]:
        """Get predictions from all models with performance tracking"""
        if not self.config.ENABLE_ML_MODELS or not self.models:
            return self._get_empty_prediction()
        
        # Check if models are trained
        if not self._is_trained:
            self.logger.info("ML ëª¨ë¸ì´ ì•„ì§ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ˆê¸° í•™ìŠµì„ ì‹œë„í•©ë‹ˆë‹¤...")
            await self._initial_training()
            if not self._is_trained:
                self.logger.warning("ì´ˆê¸° í•™ìŠµ ì‹¤íŒ¨. ê¸°ë³¸ ì˜ˆì¸¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.")
                return self._get_default_prediction()
        
        # Check if models need retraining
        await self._check_and_retrain_models()
        
        try:
            # Prepare features
            feature_array = self._prepare_features(features)
            
            if feature_array is None:
                return self._get_empty_prediction()
            
            # Get predictions from each model
            predictions = {}
            prediction_ids = {}
            
            for model_name, model in self.models.items():
                try:
                    prediction = await self._predict_with_model(
                        model_name, model, feature_array
                    )
                    predictions[model_name] = prediction
                    
                    # Save prediction for tracking
                    if prediction:
                        pred_id = self.db.save_ml_prediction(
                            model_name, symbol, prediction['prediction'],
                            prediction['confidence'], features
                        )
                        prediction_ids[model_name] = pred_id
                        
                except Exception as e:
                    self.logger.error(f"{model_name} ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
                    predictions[model_name] = None
            
            # Ensemble prediction
            ensemble_prediction = self._ensemble_predictions(predictions)
            
            return {
                'individual_predictions': predictions,
                'ensemble': ensemble_prediction,
                'features_used': self.feature_names,
                'confidence_factors': self._calculate_confidence_factors(features),
                'prediction_ids': prediction_ids
            }
            
        except Exception as e:
            self.logger.error(f"ML ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
            return self._get_empty_prediction()
    
    async def _check_and_retrain_models(self):
        """Check if models need retraining based on performance"""
        current_time = datetime.now()
        
        for model_name in self.models:
            # Check last retrain time
            last_retrain = self.last_retrain_time.get(model_name, datetime.min)
            hours_since_retrain = (current_time - last_retrain).total_seconds() / 3600
            
            # Get current performance
            perf = self.db.get_ml_model_performance(model_name, self.config.ML_PREDICTION_WINDOW)
            
            # Retrain if:
            # 1. It's been longer than configured hours
            # 2. Performance dropped below threshold
            should_retrain = (
                hours_since_retrain >= self.config.ML_RETRAIN_HOURS or
                perf['accuracy'] < self.config.ML_MIN_PERFORMANCE
            )
            
            if should_retrain and perf['total_predictions'] >= 100:
                self.logger.info(f"{model_name} ì¬í•™ìŠµ ì¤‘ - ì •í™•ë„: {perf['accuracy']:.2%}")
                await self._retrain_model(model_name)
    
    async def _retrain_model(self, model_name: str):
        """Retrain a specific model with recent data"""
        try:
            # Get training data from recent predictions and actual results
            with self.db._get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT features, actual 
                    FROM ml_performance 
                    WHERE model_name = ? AND actual IS NOT NULL
                    ORDER BY timestamp DESC 
                    LIMIT 5000
                """, (model_name,))
                
                training_data = cursor.fetchall()
            
            if len(training_data) < 1000:
                self.logger.warning(f"{model_name} ì¬í•™ìŠµì„ ìœ„í•œ ë°ì´í„° ë¶€ì¡±")
                return
            
            # Prepare training data
            X = []
            y = []
            
            for row in training_data:
                features = json.loads(row['features'])
                feature_array = [features.get(fname, 0) for fname in self.feature_names]
                X.append(feature_array)
                y.append(row['actual'])
            
            X = np.array(X)
            y = np.array(y)
            
            # Split data
            split_idx = int(len(X) * 0.8)
            X_train, X_test = X[:split_idx], X[split_idx:]
            y_train, y_test = y[:split_idx], y[split_idx:]
            
            # Retrain in background
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(
                None,
                self._train_model,
                model_name, X_train, y_train, X_test, y_test
            )
            
            self.last_retrain_time[model_name] = datetime.now()
            self.logger.info(f"{model_name} ëª¨ë¸ ì¬í•™ìŠµ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"{model_name} ì¬í•™ìŠµ ì‹¤íŒ¨: {e}")
    
    def _train_model(self, model_name: str, X_train, y_train, X_test, y_test):
        """Train model and evaluate performance"""
        model = self.models[model_name]
        
        # Fit model
        model.fit(X_train, y_train)
        self._is_trained = True
        
        # Evaluate
        predictions = model.predict(X_test)
        accuracy = np.mean(np.sign(predictions) == np.sign(y_test))
        
        self.logger.info(f"{model_name} ì¬í•™ìŠµ ì •í™•ë„: {accuracy:.2%}")
        
        # Save model
        self.save_models()
    
    async def _initial_model_training(self):
        """Initial training for new models using historical data"""
        try:
            self.logger.info("ì´ˆê¸° ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
            
            # Try to load historical data from CSV files
            csv_files = [
                "BTCUSDT_15m_1month.csv",
                "BTCUSDT_30min.csv", 
                "btcusdt_1h_dtp.csv",
                "btcusdt_30m.csv"
            ]
            
            training_data = []
            for csv_file in csv_files:
                if os.path.exists(csv_file):
                    try:
                        df = pd.read_csv(csv_file)
                        if len(df) > 100:
                            training_data.append(df)
                            self.logger.info(f"{csv_file}ì—ì„œ {len(df)}ê°œ í–‰ ë¡œë“œ")
                    except Exception as e:
                        self.logger.warning(f"{csv_file} ë¡œë“œ ì‹¤íŒ¨: {e}")
            
            if not training_data:
                self.logger.warning("ì´ˆê¸° í•™ìŠµì„ ìœ„í•œ ê³¼ê±° ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                return
            
            # Combine all data
            combined_df = pd.concat(training_data, ignore_index=True)
            self.logger.info(f"ê²°í•©ëœ í•™ìŠµ ë°ì´í„°: {len(combined_df)}ê°œ í–‰")
            
            # Generate features and targets from historical data
            X, y = await self._prepare_training_data(combined_df)
            
            if len(X) < 100:
                self.logger.warning("í•™ìŠµ ë°ì´í„° ë¶€ì¡±")
                return
            
            # Fit scalers
            self.scalers['features'].fit(X)
            X_scaled = self.scalers['features'].transform(X)
            
            # Split data
            split_idx = int(len(X) * 0.8)
            X_train, X_test = X_scaled[:split_idx], X_scaled[split_idx:]
            y_train, y_test = y[:split_idx], y[split_idx:]
            
            # Train all models
            for model_name in self.models:
                try:
                    loop = asyncio.get_event_loop()
                    await loop.run_in_executor(
                        None,
                        self._train_model,
                        model_name, X_train, y_train, X_test, y_test
                    )
                    self.logger.info(f"{model_name} ì´ˆê¸° í•™ìŠµ ì™„ë£Œ")
                except Exception as e:
                    self.logger.error(f"{model_name} ì´ˆê¸° í•™ìŠµ ì‹¤íŒ¨: {e}")
            
            self._is_trained = True
            self.logger.info("ì´ˆê¸° ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"ì´ˆê¸° ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {e}")
    
    async def _prepare_training_data(self, df):
        """Prepare training data from historical DataFrame"""
        try:
            # Calculate technical indicators
            df['returns'] = df['close'].pct_change()
            df['returns_1'] = df['returns'].shift(1)
            df['returns_5'] = df['returns'].rolling(5).mean()
            df['returns_20'] = df['returns'].rolling(20).mean()
            
            # Volume indicators
            df['volume_ma'] = df['volume'].rolling(20).mean()
            df['volume_ratio'] = df['volume'] / df['volume_ma']
            
            # Price indicators
            df['sma_20'] = df['close'].rolling(20).mean()
            df['price_ma_ratio'] = df['close'] / df['sma_20']
            
            # Simple volatility
            df['volatility'] = df['returns'].rolling(20).std()
            df['volatility_ratio'] = df['volatility'] / df['volatility'].rolling(50).mean()
            
            # Remove NaN values
            df = df.dropna()
            
            # Prepare features
            X = []
            y = []
            
            for i in range(len(df) - 1):
                features = {
                    'returns_1': df.iloc[i]['returns_1'] if not pd.isna(df.iloc[i]['returns_1']) else 0,
                    'returns_5': df.iloc[i]['returns_5'] if not pd.isna(df.iloc[i]['returns_5']) else 0,
                    'returns_20': df.iloc[i]['returns_20'] if not pd.isna(df.iloc[i]['returns_20']) else 0,
                    'volume_ratio': df.iloc[i]['volume_ratio'] if not pd.isna(df.iloc[i]['volume_ratio']) else 1,
                    'price_ma_ratio': df.iloc[i]['price_ma_ratio'] if not pd.isna(df.iloc[i]['price_ma_ratio']) else 1,
                    'volatility_ratio': df.iloc[i]['volatility_ratio'] if not pd.isna(df.iloc[i]['volatility_ratio']) else 1,
                    'atr_ratio': 0.02,  # Default values for missing indicators
                    'bb_position': 0.5,
                    'rsi': 50.0,
                    'macd_signal': 0.0,
                    'adx': 25.0,
                    'obv_slope': 0.0,
                    'volume_ma_ratio': df.iloc[i]['volume_ratio'] if not pd.isna(df.iloc[i]['volume_ratio']) else 1,
                    'trend_strength': 0.5
                }
                
                # Target: next period return
                target = df.iloc[i + 1]['returns'] * 100  # Convert to percentage
                
                # Convert features to array
                feature_array = [features.get(fname, 0) for fname in self.feature_names]
                X.append(feature_array)
                y.append(target)
            
            return np.array(X), np.array(y)
            
        except Exception as e:
            self.logger.error(f"í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì˜¤ë¥˜: {e}")
            return np.array([]), np.array([])
    
    def _prepare_features(self, raw_features: Dict[str, float]) -> Optional[np.ndarray]:
        """Prepare features for ML models"""
        try:
            # Extract features in correct order
            feature_values = []
            for feature_name in self.feature_names:
                if feature_name in raw_features:
                    feature_values.append(raw_features[feature_name])
                else:
                    # Use default values for missing features
                    feature_values.append(0.0)
            
            # Convert to numpy array
            feature_array = np.array(feature_values).reshape(1, -1)
            
            # Scale features if scaler is fitted
            if hasattr(self.scalers['features'], 'mean_'):
                feature_array = self.scalers['features'].transform(feature_array)
            
            return feature_array
            
        except Exception as e:
            self.logger.error(f"íŠ¹ì§• ì¤€ë¹„ ì˜¤ë¥˜: {e}")
            return None
    
    async def _predict_with_model(self, model_name: str, model: Any, 
                                 features: np.ndarray) -> Dict:
        """Get prediction from single model"""
        try:
            # Check if model is fitted
            if not hasattr(model, 'n_features_in_'):
                self.logger.warning(f"{model_name}ì´ ì•„ì§ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                return None
                
            # Run prediction in thread pool
            loop = asyncio.get_event_loop()
            
            # Make prediction
            prediction = await loop.run_in_executor(
                None, model.predict, features
            )
            
            # Get prediction probability/confidence if available
            confidence = 0.5
            if hasattr(model, 'predict_proba'):
                try:
                    proba = await loop.run_in_executor(
                        None, model.predict_proba, features
                    )
                    confidence = np.max(proba)
                except (AttributeError, ValueError, TypeError, Exception) as e:
                    logging.error(f"ML ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
                    confidence = 0.5
            
            # Convert prediction to direction and magnitude
            pred_value = float(prediction[0])
            
            if abs(pred_value) < 0.001:  # Near zero
                direction = 'neutral'
            elif pred_value > 0:
                direction = 'long'
            else:
                direction = 'short'
            
            return {
                'prediction': pred_value,
                'direction': direction,
                'confidence': float(confidence),
                'model': model_name
            }
            
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
            return None
    
    def _ensemble_predictions(self, predictions: Dict) -> Dict:
        """Combine predictions from multiple models with performance weighting"""
        valid_predictions = [
            p for p in predictions.values() 
            if p is not None and 'prediction' in p
        ]
        
        if not valid_predictions:
            return {
                'prediction': 0,
                'direction': 'neutral',
                'confidence': 0,
                'method': 'none'
            }
        
        # Weighted average based on model performance
        total_weight = 0
        weighted_sum = 0
        direction_votes = defaultdict(float)
        
        for pred in valid_predictions:
            # Get model weight based on historical performance
            model_name = pred['model']
            perf = self.model_performance.get(model_name, {'accuracy': 0.5})
            
            # Use accuracy as weight
            weight = max(0.1, perf['accuracy'])
            
            # Weight by confidence too
            weight *= pred['confidence']
            
            weighted_sum += pred['prediction'] * weight
            direction_votes[pred['direction']] += weight
            total_weight += weight
        
        if total_weight == 0:
            ensemble_pred = np.mean([p['prediction'] for p in valid_predictions])
            ensemble_conf = np.mean([p['confidence'] for p in valid_predictions])
        else:
            ensemble_pred = weighted_sum / total_weight
            ensemble_conf = total_weight / len(valid_predictions)
        
        # Determine direction from votes
        ensemble_direction = max(direction_votes.items(), key=lambda x: x[1])[0]
        
        return {
            'prediction': ensemble_pred,
            'direction': ensemble_direction,
            'confidence': min(ensemble_conf, 0.95),
            'method': 'weighted_ensemble',
            'model_count': len(valid_predictions)
        }
    
    def _calculate_confidence_factors(self, features: Dict[str, float]) -> Dict[str, float]:
        """Calculate factors affecting prediction confidence"""
        factors = {}
        
        # Market regime confidence
        if 'trend_strength' in features:
            factors['trend_clarity'] = abs(features['trend_strength'])
        
        # Volatility impact
        if 'atr_ratio' in features:
            # Lower confidence in high volatility
            factors['volatility_penalty'] = max(0, 1 - features['atr_ratio'] * 10)
        
        # Technical alignment
        technical_signals = ['rsi', 'macd_signal', 'bb_position']
        aligned_signals = sum(1 for sig in technical_signals 
                            if sig in features and abs(features[sig]) > 0.3)
        factors['technical_alignment'] = aligned_signals / len(technical_signals)
        
        return factors
    
    def update_prediction_result(self, prediction_id: int, actual_result: float):
        """Update prediction with actual result for learning"""
        self.db.update_ml_prediction_result(prediction_id, actual_result)
    
    def save_models(self):
        """Save models to disk"""
        model_dir = "models"
        os.makedirs(model_dir, exist_ok=True)
        
        # Save models
        for model_name, model in self.models.items():
            filepath = os.path.join(model_dir, f"{model_name}_model.pkl")
            joblib.dump(model, filepath)
        
        # Save scalers
        scaler_path = os.path.join(model_dir, 'scalers.pkl')
        joblib.dump(self.scalers, scaler_path)
        
        self.logger.info("ëª¨ë¸ ì €ì¥ ì™„ë£Œ")
    
    async def _initial_training(self):
        """Perform initial training with minimal data or synthetic data"""
        try:
            # Try to get some historical data for training
            # If no data available, use synthetic data for initialization
            self.logger.info("ì´ˆê¸° ML ëª¨ë¸ í•™ìŠµ ì¤‘...")
            
            # Create minimal synthetic training data
            n_samples = 100
            np.random.seed(42)
            
            # Generate random features
            X = np.random.rand(n_samples, len(self.feature_names))
            # Simple target: slight trend based on features
            y = (X[:, 0] - 0.5) * 0.02 + np.random.normal(0, 0.005, n_samples)
            
            # Fit scalers
            self.scalers['features'].fit(X)
            self.scalers['target'].fit(y.reshape(-1, 1))
            
            # Train models with synthetic data
            X_scaled = self.scalers['features'].transform(X)
            y_scaled = self.scalers['target'].transform(y.reshape(-1, 1)).ravel()
            
            trained_count = 0
            for model_name, model in self.models.items():
                try:
                    model.fit(X_scaled, y_scaled)
                    trained_count += 1
                    self.logger.info(f"{model_name} ì´ˆê¸° í•™ìŠµ ì™„ë£Œ")
                except Exception as e:
                    self.logger.error(f"{model_name} ì´ˆê¸° í•™ìŠµ ì‹¤íŒ¨: {e}")
            
            if trained_count > 0:
                self._is_trained = True
                self.logger.info(f"{trained_count}ê°œ ëª¨ë¸ ì´ˆê¸° í•™ìŠµ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"ì´ˆê¸° í•™ìŠµ ì˜¤ë¥˜: {e}")
    
    def _get_default_prediction(self) -> Dict:
        """Return default prediction when models are not available"""
        return {
            'individual_predictions': {},
            'ensemble': {
                'prediction': 0.001,  # Slightly positive bias
                'direction': 'neutral',
                'confidence': 0.3,  # Low confidence
                'method': 'default'
            },
            'features_used': [],
            'confidence_factors': {'default_mode': True},
            'prediction_ids': {}
        }
    
    def _get_empty_prediction(self) -> Dict:
        """Return empty prediction structure"""
        return {
            'individual_predictions': {},
            'ensemble': {
                'prediction': 0,
                'direction': 'neutral',
                'confidence': 0,
                'method': 'none'
            },
            'features_used': [],
            'confidence_factors': {},
            'prediction_ids': {}
        }
    
    async def _prediction_verification_loop(self):
        """ì£¼ê¸°ì ìœ¼ë¡œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ê²€ì¦í•˜ëŠ” ë£¨í”„"""
        while True:
            try:
                await asyncio.sleep(300)  # 5ë¶„ë§ˆë‹¤ ì‹¤í–‰
                
                # 24ì‹œê°„ ì´ìƒ ì§€ë‚œ ì˜ˆì¸¡ë“¤ì„ ê²€ì¦
                cutoff_time = datetime.now().timestamp() - 86400  # 24ì‹œê°„ ì „
                
                # ë¯¸ê²€ì¦ ì˜ˆì¸¡ë“¤ ê°€ì ¸ì˜¤ê¸°
                unverified_predictions = await self._get_unverified_predictions(cutoff_time)
                
                for prediction in unverified_predictions:
                    await self._verify_single_prediction(prediction)
                
                # ëª¨ë¸ ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
                await self._update_model_performance_stats()
                
            except Exception as e:
                self.logger.error(f"âŒ ì˜ˆì¸¡ ê²€ì¦ ë£¨í”„ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(60)
    
    async def _get_unverified_predictions(self, cutoff_time: float) -> List[Dict]:
        """ê²€ì¦ë˜ì§€ ì•Šì€ ì˜ˆì¸¡ë“¤ì„ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê°€ì ¸ì˜¤ê¸°"""
        try:
            # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¯¸ê²€ì¦ ì˜ˆì¸¡ ì¡°íšŒ
            with self.db._get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT id, symbol, prediction, confidence, timestamp
                    FROM ml_performance 
                    WHERE actual IS NULL 
                    AND timestamp < ?
                    ORDER BY timestamp DESC
                    LIMIT 100
                """, (cutoff_time * 1000,))  # milliseconds
                
                return [dict(row) for row in cursor.fetchall()]
                
        except Exception as e:
            self.logger.error(f"âŒ ë¯¸ê²€ì¦ ì˜ˆì¸¡ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    async def _verify_single_prediction(self, prediction: Dict):
        """ë‹¨ì¼ ì˜ˆì¸¡ ê²°ê³¼ ê²€ì¦"""
        try:
            symbol = prediction['symbol']
            prediction_value = prediction['prediction']
            prediction_time = prediction['timestamp'] / 1000  # Convert to seconds
            
            # ì˜ˆì¸¡ ì‹œì  ì´í›„ì˜ ì‹¤ì œ ê°€ê²© ë³€í™” ê³„ì‚°
            actual_change = await self._get_actual_price_change(symbol, prediction_time)
            
            if actual_change is not None:
                # ë°©í–¥ ì˜ˆì¸¡ ì •í™•ë„ ê³„ì‚°
                direction_correct = (prediction_value > 0 and actual_change > 0) or \
                                  (prediction_value < 0 and actual_change < 0)
                
                # ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸
                self.db.update_ml_prediction_result(prediction['id'], actual_change)
                
                self.logger.debug(f"ğŸ” {symbol} ì˜ˆì¸¡ ê²€ì¦: ì˜ˆì¸¡={prediction_value:.4f}, ì‹¤ì œ={actual_change:.4f}, ë°©í–¥ì¼ì¹˜={direction_correct}")
                
        except Exception as e:
            self.logger.error(f"âŒ ì˜ˆì¸¡ ê²€ì¦ ì˜¤ë¥˜: {e}")
    
    async def _get_actual_price_change(self, symbol: str, prediction_time: float) -> Optional[float]:
        """ì˜ˆì¸¡ ì‹œì  ì´í›„ì˜ ì‹¤ì œ ê°€ê²© ë³€í™” ê³„ì‚°"""
        try:
            # ì˜ˆì¸¡ ì‹œì ê³¼ 24ì‹œê°„ í›„ì˜ ê°€ê²© ë°ì´í„° ì¡°íšŒ
            start_time = prediction_time
            end_time = prediction_time + 86400  # 24ì‹œê°„ í›„
            
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ê±°ë˜ì†Œì—ì„œ ê³¼ê±° ê°€ê²© ë°ì´í„°ë¥¼ ì¡°íšŒí•´ì•¼ í•¨
            # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ êµ¬í˜„
            if hasattr(self, 'exchange') and self.exchange:
                current_price = self.exchange.get_current_price(symbol)
                if current_price:
                    # ì„ì‹œ: í˜„ì¬ ê°€ê²© ê¸°ì¤€ìœ¼ë¡œ ëœë¤ ë³€í™”ìœ¨ ìƒì„± (ì‹¤ì œë¡œëŠ” ê³¼ê±° ë°ì´í„° ì¡°íšŒ)
                    return np.random.normal(0, 0.02)  # í‰ê·  0, í‘œì¤€í¸ì°¨ 2% ë³€í™”
            
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ ì‹¤ì œ ê°€ê²© ë³€í™” ê³„ì‚° ì˜¤ë¥˜: {e}")
            return None
    
    async def _update_model_performance_stats(self):
        """ëª¨ë¸ ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸"""
        try:
            for symbol in self.config.SYMBOLS:
                performance = self.db.get_ml_model_performance(f"ensemble_{symbol}")
                self.model_performance[symbol] = performance
                
                if performance['total_predictions'] > 0:
                    self.logger.info(f"ğŸ“Š {symbol} ML ì„±ëŠ¥: ì •í™•ë„={performance['accuracy']:.1%}, ì˜ˆì¸¡ìˆ˜={performance['total_predictions']}")
                    
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")